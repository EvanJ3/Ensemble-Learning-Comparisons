# Ensemble-Learning-Comparisons
A comparative analysis of tree learners on Istanbul Stock Exchange Data


## 1 Introduction
The strength of tree-classifiers derives from their ability to represent complex data distributions via the application of many representationally simple classifi-ers. Through combining these decision rules, that alone would be rather un-informative or far too elementary, together they provide an extremely flexible method for fitting data. However, this flexibility does come at a cost, and as we will see throughout this report, these types of models have a high propensity to overfit training data if hyperparameters are not selected properly. Ensemble methods such as Bootstrap Aggregation or “bagging” have been proposed as a remedy for such drawbacks. By fitting many individual tree learners on different samplings of the training data then grouping them together through out-put/prediction aggregation (via averaging), bagging aims to reduce overall vari-ance and improve stability compared to single tree-based models (Breiman 1996). To best evaluate the performance profiles of decision, random, and bagged tree learners we will conduct four experiments each of which is designed to test the following hypotheses based upon the existing conceptual knowledge of these types of tree-based learners: Firstly, decision tree models are especially prone to overfitting if the hyperparameter of choice maximum leaf size is sufficiently small. In such a situation the model will perform poorly on held-out test data de-spite originating from the same distribution of the training data. Secondly, bag-ging in general is a moderately effective augmentation to existing tree-based models for reducing overfitting with respect to leaf size, but despite their best efforts for extreme small values of maximum leaf size they will be unable to elim-inate overfitting entirely. Thirdly, decision tree models will perform moderately better than random trees on in/out sample error, but random methods will be more likely to underfit as opposed to overfit. Lastly, random tree models will require considerably less time to train that decision trees, but performance on time to predict will be nearly equal. With each of these hypotheses in mind, we now move onto the design of our experiments which will provide empirical vali-dation or invalidation to each respective hypothesis. 

## 2 Methods

In this report we will discuss several tree classifiers and their performance char-acteristics on the UCL Istanbul Stock Exchange dataset. This dataset utilizes daily returns of international stock indexes as features and the return of Istanbul’s stock exchange for the respective day as target labels. Each learner is written in Python 3 and utilizes a NumPy array-based representation of the respective model trees (Quinlan 1986). A random number generator with seed set to 901132254 is used for ease of replication. The dataset will be divided up into training and testing subsets according to a 60/40 split with random assignment without replacement. For each of the following experiments, the models will be trained independently on this same training and test data. For each of the three classes of model, 100 different models of each with varying hyper-parameter maximum leaf size were trained. Further important implementational details to specific leaners will be discussed within their respective experiment sections.

## 3	EXPERIMENTAL RESULTS & DISCUSSION:

### 3.1	Experiment 1:
In this first experiment one-hundred different decision tree learners were trained according to a unique leaf size and identical dataset. This particularly implemen-tation of the decision tree learner utilized the greatest absolute value of feature to label correlation as the split feature selection criterion and a median split val-ue criterion. Features that demonstrated equal absolute correlation were selected on a first appearance basis in the event of ties. After training each model, each was queried for prediction on both the feature data used during training as well as the held-out test feature data. These output predictions are then compared with their respective target labels via the residual mean squared error (RMSE) metric to arrive at values for the model’s in and out sample errors, respectively. This process was applied to each of the one-hundred models and were plotted alongside one another in Figure 1.1 according to error type. Additionally, to demonstrate a clearer picture of overfitting, the difference of each model’s out-sample RMSE and in-sample RMSE was taken and plotted in Figure 1.2. By tak-ing the difference of these two metrics a more informative picture of where a model is overfitting can be observed. Places where the RMSE difference is large indicate areas of strong overfitting as our test data had far more error than ob-served in the training data. When this is observed it is often an indicator that our model overfit our training-set and thus was unable to perform similarly when faced with new un-seen distributionally similar data in the test set. Both plots illustrate that for small values of leaf size (1 ≤ x ≤ 10) the model performs much stronger on the training set than the test set and thus indicates overfitting. This is especially pronounced when leaf size = 1 which is expected as this choice of leaf size directly “memorizes” the training set by making each observation its own leaf (with a minor exception of edge cases). As leaf size increases the effect of overfitting is diminished with strong improvement from [1,20) and positive yet diminishing improvement from (20 ,100]. This behavior matches with that pro-posed in the first hypothesis. However, it is important to note that although in-creased leaf size reduces overfitting, it directly limits our ability to form more complex representations of the data as there are exponentially fewer split points, which reduces overall model capacity both for in-sample and out-sample data. Thus, like most choices of hyperparameters, there is an inherent tradeoff of bias and variance. As is typically the case, the middle ground is likely to be the best choice which we found to be around the value of ~20 for the maximum leaf size parameter.

### 3.2	Experiment 2:
In this experiment one-hundred models of varying leaf size for a bagged decision tree learner with 20 bags were trained. The bag size of 20 was selected via ad-hoc experimentation and was found to be of little impact to overall model per-formance. Each of the models within the bagged leaner utilized the same imple-mentation of the decision tree model utilized in experiment 1. Each model within the bag learner was sampled with replacement from the training set inde-pendently and was trained upon their respective resulting sample of equal size to the number of training observations. In evaluating the premise that via boot-strapping data and aggerating several models will reduce overall variance and improve stability of standalone decision tree models; we constructed this exper-iment to directly compare their performance with a standard decision tree model used as a benchmark. In Figure 2.1, the RMSE of each model’s respective in-sample and out-sample errors were recorded. Figure 2.2 similarly depicts the difference of the out-sample and in-sample RMSE for each respective model type for a consolidated view of overfitting. As we can see clearly from both plots, bagging gives a consistent slight reduction in both types of error. However, this reduction is especially evident for lower values of leaf size most prominently 1. Overall, our results indicate that our second hypothesis is proved valid. The bagged decision tree model indicates a slight reduction of overfitting at compa-rable leaf sizes to a standalone decision tree learner. Although the problem of overfitting with re-spect to leaf size can be reduced it cannot and is not eliminated entirely. 

### 3.3 Experiment 3:
In contrast to experiments 1 & 2 which utilized RMSE as the metric for compar-ing in/out sample error in this experiment, the sum of absolute errors (L1) will be used to evaluate their performance. The reason for the change in metric has to do with the type of data we are predicating which are decimal values mostly in the range of (-1,1). The RMSE metric is less than ideal for measuring error on this type of labeled data. This is in part due to the squaring operation which cre-ates a situation where small errors are penalized to a greater degree than larger magnitude errors. To remedy this, the error metric of sum of absolute differ-ences is proposed as an alternative which penalizes small and large errors in a more equal fashion. The results of applying this new error metric to the in/out sample data is demonstrated in Figure 3.1. The difference of out-sample & in-sample error is again depicted for ease of visualization in Figure 3.2. The plots collectively demonstrate that under the new L1 error metric, there is still a high propensity for overfitting in both decision and random tree models for small leaf size. It is notable that for the first time the in-sample error for both decision and random trees is greater (for leaf sizes (10,100)) than out-sample error thus indi-cating a slight amount of underfitting to the training data. This contrasts with the RMSE view of decision tree error which shows that fit to training data for leaf sizes (20,100] is neutral to fit whereas under the L1 error it is neutral to under-fit. The observation demonstrates the importance of selecting a proper error metric when evaluating models as different metrics may provide an alternative view of the model behavior. In both plots we can see that the random tree learner shows consistently worse performance than the decision tree learner in L1 error. This makes sense as the injected randomness in split points is less informative than the criterion selected by the decision learner leading to worse fit of the training data. Ideally, the random tree model would use this underfitting of training data to generalize better to the out-sample data. However, as we can see this is only the case for small leaf sizes in the range of [1,7) where the random tree model demonstrates less overfitting, but error remains on-par with that of the decision tree model. With these conflicting observations in mind, it is impossible to say that one learner is unequivocally better than the other in this area of perfor-mance. Although random trees demonstrate less propensity to over-fit than deci-sion trees, their error is still in-line if not larger both in and out of sample. Addi-tionally, this underfitting improvement of random trees over decision trees is only valid for small leaf sizes and quickly becomes a problem of under-fitting the training set as leaf size increases which deteriorates corresponding out-sample performance, which matches with our third hypothesis. With these conclusions in mind, it is important to note that random trees may perform far better than decision trees (and vice versa) under different datasets and distributions which should be explored in future research.

### 3.4 Experiment 4:
In this final experiment we continue to evaluate the performance of decision and random tree models with respect to compute time. To evaluate training time, each model was timed seven times per run, per leaf, and then averaged. Since our test set was relatively small, 10,000 fake random observations were generat-ed to use as our inference data. The data was generated in this fashion and amount to create clearly discernable differences in our results as the test set was far too small to provide a reasonably measured clock time. The results of the training and inference experiments are demonstrated in Figures 4.1 & 4.2, re-spectively. Since the decision tree implementation used for this report involves using an absolute correlation and median operation to determine split points. These operations are far more expensive when compared to the random imple-mentation which requires only a random number generator. In Figure 4.1 we directly observe this behavior especially for small leaf sizes where the number of nodes is substantially larger. For leaf sizes in the range of (40,100] there is little to no difference between the training times as the number of nodes is relatively small. In the case of Figure 4.2, we notice similar behavior as the random learner consistently proves quicker due to an on average smaller look-up table. This shows that the random model was more effectively splitting the data. We can soundly conclude that with respect to training and inference time, random tree models consistently out-perform decision models. These observations confirm our expectations presented in the fourth hypothesis. However, together with the results from experiment 3 it is rather ambiguous as to if one model is better than the other. Neither model demonstrates strictly un-dominated performance across all metrics. Thus, this modeling decision should be approached in a more data and task specific light in future research. 

## 4 SUMMARY:

In this report we explored through four experiments the performance character-istics of three widely used tree-based classification and regression models. As was evident by our results, this family of models has excellent potential for ap-plications in predication, and especially thrives on both simplicity and flexibility. However, as we also observed hyper parameter selection can be a crucial choice to train a model that generalizes well to un-seen data. Although random trees and bagging improve upon the base case decision tree, they are still prone to these complications just to a lesser degree. Overall, tree-based models represent an extremely flexibly and adaptive set of models.

## 5	REFERENCES
1.	Breiman, L. (1996). Bagging predictors. Machine learning, 24(2), 123-140.
2.	Quinlan, J. R. (1986). Induction of decision trees. Machine learning, 1(1), 81-106.


